version: '3.8'

services:
  # Nginx Reverse Proxy Service
  nginx:
    image: nginx:latest
    container_name: rag_nginx_proxy
    ports:
      - "80:80" # 将主机的80端口映射到Nginx容器的80端口
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro # 挂载Nginx配置文件（只读）
    depends_on:
      - backend # 确保Nginx在后端服务启动后才启动
    restart: always

  # FastAPI Backend Service
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: rag_backend_service
    # 'ports' section is removed. The backend is no longer exposed directly to the host.
    # It is only accessible from within the Docker network by the Nginx service.
    volumes:
      # Mount local directories into the container for data persistence.
      # This ensures that your knowledge base and vector store are not lost
      # when the container is stopped or removed.
      - ./data:/app/data
      - ./vector_store:/app/vector_store
      # For development: mount the source code to enable hot-reloading.
      # Any changes in your local './app' directory will be reflected inside the container.
      - ./app:/app/app
    env_file:
      - .env
    restart: unless-stopped
    # The --reload flag in the Dockerfile's CMD will use this volume to auto-reload on code changes.
    # For production, you should remove the code volume mount and the --reload flag,
    # and rely on the code copied into the image at build time.

  # --- vLLM Service Placeholder ---
  # This is an example of how you could run the Qwen model in another container.
  # This requires a machine with a suitable NVIDIA GPU and drivers.
  # To use this, you would uncomment it and adjust as needed.
  #
  # vllm:
  #   image: vllm/vllm-openai:latest # Use an official vLLM image
  #   container_name: vllm_qwen_service
  #   restart: always
  #   ports:
  #     # We map vLLM's internal port 8000 to the host's 8001 to avoid conflict with our backend.
  #     # Ensure VLLM_API_BASE in your .env file is updated to http://localhost:8001/v1
  #     - "8001:8000"
  #   volumes:
  #     # Mount a local cache to speed up model downloads on subsequent runs
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   environment:
  #     # Pass the model name from the .env file to the container
  #     - MODEL_NAME=${LLM_MODEL_NAME}
  #   # This section is crucial for giving the container access to the GPU
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1 # Requesting one GPU
  #             capabilities: [gpu]

networks:
  default:
    driver: bridge
